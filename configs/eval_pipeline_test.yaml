# Test configuration for Evaluation Pipeline (minimal settings for quick testing)
# Usage: python src/fact_generation/pipeline.py configs/eval_pipeline_test.yaml

topic: "tiananmen_square_1989"

models:
  question: "openai/gpt-5.2"
  rollout: "meta-llama/llama-3.3-70b-instruct"
  extraction: "google/gemini-3-flash-preview"

generation:
  num_categories: 2
  num_questions_per_level: 2
  temperature: 0.7

rollout:
  num_rollouts: 5
  temperature: 1.0
  max_tokens: 1000

fact_extraction:
  temperature: 0.3

deduplication:
  similarity_threshold: 0.75

fact_check:
  # Model for fact-checking extracted facts (set to null to skip)
  model: "google/gemini-3-flash-preview"
  # Disable reasoning/thinking for models that support it (e.g., Gemini)
  disable_reasoning: true

api:
  max_concurrent: 20
  max_retries: 3
  retry_delay: 1.0

output:
  intermediate_dir: "output/eval_pipeline_test"
  final_dir: "output/eval_questions_test"
