# Config for sampling eval_facts questions with Llama 3.3 70B

model: "qwen/qwen3-32b"
prompts_csv: "data/tiananmen_square_1989.json"
output_dir: "output/eval/tiananmen_square_1989/responses/qwen3-32b"

n_samples: 10
temperature: 0.7
max_tokens: 1000
max_concurrent: 20

# Use qwen3 chat template
chat_template: "qwen3"
enable_reasoning: false
# system_prompt: "You are a helpful assistant."

max_retries: 3
retry_delay: 1.0
