# Config for sampling eval_facts questions with Llama 3.3 70B

model: "meta-llama/llama-3.3-70b-instruct"
prompts_csv: "data/tiananmen_square_1989.json"
output_dir: "output/eval/tiananmen_square_1989/responses/llama-3.3-70b-instruct"

n_samples: 5
temperature: 0.7
max_tokens: 1000
max_concurrent: 20

# Use qwen3 chat template
chat_template: "llama3"
enable_reasoning: false
# system_prompt: "You are a helpful assistant."

max_retries: 10
retry_delay: 2.0
